import os
import torch
import asyncio
import logging
from flask import Flask, request
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram.constants import ParseMode
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# Logging ‡§∏‡•á‡§ü‡§Ö‡§™
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ================================================================
# 1. ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§® (Configuration)
# ================================================================
TOKEN = os.environ.get('TELEGRAM_TOKEN')
if not TOKEN:
    logger.error("TELEGRAM_TOKEN ‡§è‡§®‡§µ‡§æ‡§Ø‡§∞‡§®‡§Æ‡•á‡§Ç‡§ü ‡§µ‡•á‡§∞‡§ø‡§è‡§¨‡§≤ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ!")

# Hugging Face ‡§™‡§∞ ‡§Ü‡§™‡§ï‡•á ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§ø‡§è ‡§π‡•Å‡§è ‡§Æ‡•â‡§°‡§≤ ‡§ï‡§æ ‡§®‡§æ‡§Æ
BASE_MODEL_ID = "google/gemma-7b"
ADAPTER_MODEL_ID = "Amitsaka1/gemma-7b-coding-assistant" # <--- ‡§Ø‡§π ‡§Ü‡§™‡§ï‡§æ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•Å‡§Ü ‡§Æ‡•â‡§°‡§≤ ‡§π‡•à

# ================================================================
# 2. AI ‡§Æ‡•â‡§°‡§≤ ‡§î‡§∞ ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡§∞ ‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡§æ
# ================================================================
print("üß† AI ‡§ï‡§æ ‡§¶‡§ø‡§Æ‡§æ‡§ó (Gemma 7B + Your Brain) ‡§≤‡•ã‡§° ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à...")
try:
    # QLoRA ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # ‡§¨‡•á‡§∏ ‡§Æ‡•â‡§°‡§≤ (Gemma 7B) ‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡§æ
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    # ‡§ü‡•ã‡§ï‡§®‡§æ‡§á‡§ú‡§º‡§∞ ‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡§æ
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
    tokenizer.pad_token = tokenizer.eos_token
    
    # ‡§Ü‡§™‡§ï‡•á ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§ø‡§è ‡§π‡•Å‡§è '‡§∏‡•ç‡§ï‡§ø‡§≤ ‡§ö‡§ø‡§™‡•ç‡§∏' (LoRA Adapters) ‡§ï‡•ã ‡§¨‡•á‡§∏ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡§ø‡§≤‡§æ‡§®‡§æ
    model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL_ID)
    
    print("‚úÖ AI ‡§ï‡§æ ‡§¶‡§ø‡§Æ‡§æ‡§ó ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§≤‡•ã‡§° ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à!")
except Exception as e:
    logger.error(f"‡§Æ‡•â‡§°‡§≤ ‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")
    model = None

# ================================================================
# 3. AI ‡§∏‡•á ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡§®‡•á ‡§ï‡§æ ‡§´‡§Ç‡§ï‡•ç‡§∂‡§®
# ================================================================
async def ask_ai(prompt):
    if not model:
        return "‡§Æ‡§æ‡§´ ‡§ï‡•Ä‡§ú‡§ø‡§è, AI ‡§Æ‡•â‡§°‡§≤ ‡§Ö‡§≠‡•Ä ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§"
    
    # ‡§™‡•ç‡§∞‡•â‡§Æ‡•ç‡§™‡•ç‡§ü ‡§ï‡•ã ‡§∏‡§π‡•Ä ‡§´‡•â‡§∞‡•ç‡§Æ‡•á‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§®‡§æ
    full_prompt = f"### Instruction:\n{prompt}\n\n### Response:\n"
    
    # ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ï‡•ã ‡§®‡§Ç‡§¨‡§∞‡•ç‡§∏ (‡§ü‡•ã‡§ï‡§®‡•ç‡§∏) ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§®‡§æ
    inputs = tokenizer(full_prompt, return_tensors="pt").to("cuda")
    
    # AI ‡§∏‡•á ‡§ú‡§µ‡§æ‡§¨ ‡§ú‡•á‡§®‡§∞‡•á‡§ü ‡§ï‡§∞‡§µ‡§æ‡§®‡§æ
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    # ‡§ú‡§µ‡§æ‡§¨ ‡§ï‡•ã ‡§µ‡§æ‡§™‡§∏ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§®‡§æ
    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ‡§∏‡§ø‡§∞‡•ç‡§´ AI ‡§ï‡§æ ‡§ú‡§µ‡§æ‡§¨ ‡§µ‡§æ‡§≤‡§æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§®‡§ø‡§ï‡§æ‡§≤‡§®‡§æ
    return response_text.split("### Response:")[1].strip()

# ================================================================
# 4. ‡§ü‡•á‡§≤‡•Ä‡§ó‡•ç‡§∞‡§æ‡§Æ ‡§π‡•à‡§Ç‡§°‡§≤‡§∞‡•ç‡§∏
# ================================================================
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡§æ ‡§´‡§æ‡§á‡§®-‡§ü‡•ç‡§Ø‡•Ç‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•Å‡§Ü Gemma 7B ‡§ï‡•ã‡§°‡§ø‡§Ç‡§ó ‡§Ö‡§∏‡§ø‡§∏‡•ç‡§ü‡•á‡§Ç‡§ü ‡§π‡•Ç‡§Å‡•§")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_text = update.message.text
    chat_id = update.effective_chat.id
    logger.info(f"'{chat_id}' ‡§∏‡•á ‡§Æ‡•à‡§∏‡•á‡§ú ‡§Ü‡§Ø‡§æ: '{user_text}'")
    
    # "‡§∏‡•ã‡§ö ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å..." ‡§Æ‡•à‡§∏‡•á‡§ú ‡§≠‡•á‡§ú‡§®‡§æ
    thinking_message = await update.message.reply_text("üß† ‡§∏‡•ã‡§ö ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å...")
    
    # AI ‡§∏‡•á ‡§ú‡§µ‡§æ‡§¨ ‡§™‡§æ‡§®‡§æ
    ai_response = await ask_ai(user_text)
    
    # "‡§∏‡•ã‡§ö ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å..." ‡§Æ‡•à‡§∏‡•á‡§ú ‡§ï‡•ã ‡§è‡§°‡§ø‡§ü ‡§ï‡§∞‡§ï‡•á ‡§´‡§æ‡§á‡§®‡§≤ ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§®‡§æ
    await context.bot.edit_message_text(chat_id=chat_id, message_id=thinking_message.message_id, text=f"```\n{ai_response}\n```", parse_mode=ParseMode.MARKDOWN_V2)

# ================================================================
# 5. Flask ‡§µ‡•á‡§¨ ‡§è‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§∂‡§® ‡§î‡§∞ ‡§¨‡•â‡§ü ‡§ï‡§æ ‡§∏‡•á‡§ü‡§Ö‡§™
# ================================================================
app = Flask(__name__)
application = Application.builder().token(TOKEN).build()
application.add_handler(CommandHandler("start", start))
application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

@app.route(f'/{TOKEN}', methods=['POST'])
def respond():
    asyncio.run(application.process_update(Update.de_json(request.get_json(force=True), application.bot)))
    return 'ok'

@app.route('/setwebhook')
def set_webhook():
    host_url = request.url_root.replace("http://", "https://")
    webhook_url = f'{host_url}{TOKEN}'
    asyncio.run(application.bot.set_webhook(webhook_url))
    return f"Webhook setup ok for {webhook_url}"

@app.route('/')
def index():
    return 'Server is running...'
    
